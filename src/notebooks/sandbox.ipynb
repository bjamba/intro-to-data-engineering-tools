{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03545808-d95c-4891-a1bb-fc555eb3a2e5",
   "metadata": {},
   "source": [
    "# Data Tooling 101 Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca2beb2-dd52-44a2-bf03-736c7a8c13b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **What the heck is a Jupyter Notebook?**\n",
    "A Jupyter Notebook is an interactive code environment that allows you to compartmentalize your code (and even markdown like this!!!) within the context of \"cells\". Code can be separated and executed in any order, on demand, within a specific cell's scope, but whatever gets executed affects a global scope, allowing you to create new global variables and update them in any order you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39b6065-d56d-4e07-a5b8-7d562a193b16",
   "metadata": {},
   "source": [
    "### 1. Executing Cells out of Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66444007-8690-45be-8b22-3c07d85f33a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute this first\n",
    "# Execute this third\n",
    "try:\n",
    "    print(an_uninstantiated_variable)\n",
    "except:\n",
    "    print(\"Your variable is uninstantiated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eccc76-5dc6-4ebd-9e77-6d0765dd1d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Execute this second\n",
    "an_uninstantiated_variable = \"I'm instantiated now!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93642cd9-f0ff-4be4-b555-fb7a30acf7b7",
   "metadata": {},
   "source": [
    "#### **Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c4733-5f89-46ed-9a5f-ed7147d07ae5",
   "metadata": {},
   "source": [
    "Using what you know about Jupyter Notebooks, create a better implementation of the `get_fibonacci_term` implementation provided below as `get_fibonacci_term_faster`. Both functions MUST execute using the timeit decorator, side-by-side, so that the print statements show up next to each other.\n",
    "\n",
    "Here are some important limitations of this exercise:\n",
    "* You can only modify the cell that is marked \"Exercise Solution\"\n",
    "* Both implementations must be called by the cell that is marked \"Exercise Validation\"\n",
    "* You can otherwise execute these cells in any order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11cdb3-a3dc-4169-8580-9080dce0d1cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def timeit(func):\n",
    "    import time\n",
    "    import traceback\n",
    "    def wrapper(*args, **kwargs):\n",
    "        before = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        after = time.time()\n",
    "        print(f\"{func.__name__} took {after - before} seconds to finish.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@timeit\n",
    "def get_fibonacci_term(n):\n",
    "    def recursive(n):\n",
    "        if n <= 2:\n",
    "            return 1\n",
    "        return recursive(n - 1) + recursive(n - 2)\n",
    "    return recursive(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363e182-f39a-4ad1-87b4-0a91bc6c4390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "funcs = [get_fibonacci_term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a652f29-c852-4487-9f3b-8aa2e37acbc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercise Validation\n",
    "for func in funcs:\n",
    "    print(func(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d605ea8-18f4-4ac0-a5d8-5ca0375fa74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercise Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c6c9d-d115-4bf3-928b-2c69b5e247ae",
   "metadata": {},
   "source": [
    "### 2. Intro to NumPy and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df029329-3036-44c7-8e47-a1b90715264c",
   "metadata": {},
   "source": [
    "NumPy (commonly aliased as `np`) and Pandas (commonly aliased as `pd`) are two of the most, if not the most, essential modules for doing data science work in Python. Consider learning them a hard requirement if you ever want to work with big data. That's NOT an exaggeration.\n",
    "\n",
    "Python has many optimizations that make it much more performant than Javascript when it comes to array-like data structures. Javascript has strings, arrays, and objects. Similarly, Python has strings, lists, and dictionaries. (There are many more examples but these are the first three comparisons people think of). That being said, Python is a higher-level language and won't be nearly as performant as a Java, C, or C++ when it comes to raw optimization of very large datasets.\n",
    "\n",
    "To address this, NumPy and Pandas leverage C and C++ under the hood in order to offload the computational grunt work onto those lower level languages. They also introduce three new and very important data structures that build off of our mental model of lists and dictionaries:\n",
    "\n",
    "1. array (NumPy): A NumPy array is a single-to-multi dimensional container that holds elements of the same data type. NumPy arrays are used for efficient storage, manipulation, and computation of large amounts of numerical data.\n",
    "\n",
    "2. Series (Pandas): A Pandas Series is a one-dimensional labeled data structure provided by the pandas library in Python. It's similar to a column in a spreadsheet or a one-column database table. A Series consists of an ordered sequence of values and associated index labels, which can be used for efficient data manipulation and analysis.\n",
    "\n",
    "3. DataFrame (Pandas): A Pandas DataFrame is a two-dimensional labeled data structure provided by the pandas library in Python. It's akin to a table in a relational database or a spreadsheet, where data is organized into rows and columns. DataFrames are versatile and widely used for data manipulation, analysis, and exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07d9a1-e6b4-4f3e-8cfd-47a47fc68339",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42275ef-481c-46a2-8e58-153202d9b04e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generating a large list v. a large array of random numbers and multipling it by 2.\n",
    "l = [random.random()*100 for _ in range(int(1e7))]\n",
    "a = np.array(l)\n",
    "\n",
    "@timeit\n",
    "def multiply_by_n(d,n):\n",
    "    if type(d) == list:\n",
    "        return map(lambda x: x * 2, d)\n",
    "    elif type(d) == np.array:\n",
    "        return d * n\n",
    "    return d\n",
    "    \n",
    "for d in [l,a]:\n",
    "    multiply_by_n(d, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b41afc-9f8d-40c0-9b33-94e7272e6325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe from a list of objects\n",
    "template = {\n",
    "    'color': ['red','green','blue','orange','black','white','gray'],\n",
    "    'car_type': ['compact','sedan','truck','suv','crossover','van','hatchback'],\n",
    "    'fuel_type': ['gas','diesel','electric','hybrid','hydrogen'],\n",
    "    'year': [2023,2022,2021,2020,2019,2018,2017],\n",
    "    'price_range': [10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000],\n",
    "    'condition': ['new', 'used'],\n",
    "    'finance_options': ['cash','finance','lease'],\n",
    "    'make': ['toyota','honda','ford','subaru','chevrolet','kia','gm']\n",
    "}\n",
    "\n",
    "def create_randomized_cars_dataset(n):\n",
    "    dataset = []\n",
    "    for _ in range(int(n)):\n",
    "        d = {}\n",
    "        for key in template.keys():\n",
    "            d[key] = template[key][random.randint(0,len(template[key])-1)]\n",
    "        dataset.append(d)\n",
    "    \n",
    "    return pd.DataFrame(dataset)\n",
    "\n",
    "# head(n) gives us the first n rows of the dataframe\n",
    "df = create_randomized_cars_dataset(1e4)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86afcb-7d53-4247-b964-f16d0699a701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A series would represent a single column or row of a dataframe\n",
    "df.car_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4870396-9a29-4d5f-91a3-442ba055dc4c",
   "metadata": {},
   "source": [
    "### 3. Applying ETL Processes to a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97109c34-9a9f-4cae-a6e6-a87eff0b009d",
   "metadata": {},
   "source": [
    "In the data engineering world, a key acronym that describes the key components of data pipelining is ETL, or Extract, Transform, and Load.\n",
    "\n",
    "1. Extract: Grabbing data from one or more sources of data (typically but not always external) to serve as the raw base data for you to change as part of a data pipeline.\n",
    "\n",
    "2. Transform: How are we going to combine and process the data we've extracted so that it provides new value to our stakeholders? Common transformations might include filtering, merging, aggregating, and labeling, among others.\n",
    "\n",
    "3. Load: Taking the data that we extracted and/or transformed and delivering it to some internal resource we own, that can be accessed by key stakeholders within our team, department, or organization.\n",
    "\n",
    "ETL processes can be ad-hoc or periodic. They can be manually executed or scheduled. They could involve local machines, bare metal servers, or cloud-based resources. They can be in the form of databases, CSVs, or flat files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9cc64-7c12-4884-8f44-15428db648b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's extract a sample dataset of cars data saved to this repo.\n",
    "# We could generate this dynamically, but this static file will\n",
    "# ensure that we get reproducible results.\n",
    "\n",
    "path = '../data/sample_cars.csv'\n",
    "cars = pd.read_csv(path)\n",
    "cars.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3303c150-3073-4c4f-8d35-3fcd36aa107f",
   "metadata": {},
   "source": [
    "#### 3.1: Filtering DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b71fb-5be2-44f1-a020-f0820c050507",
   "metadata": {},
   "source": [
    "As the name implies, filtering allows you to view only a subset of your dataframe based on a set of logical conditions. The most popular way that folks use filtering is by a concept called \"masking\", where you apply a boolean operation to your DataFrame, which results in a DataFrame of Trues and/or Falses depending on whether the condition is matched. We then mask our original DataFrame based on that condition, and only those rows/columns with True will remain viewable.\n",
    "\n",
    "As you get more comfortable working with DataFrames, there is also a `.query()` syntax that will allow you to pass in strings that have logical conditions in them that conform more to Python syntax, and which references columns directly by their names, and not via dot or square bracket notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c29e70-6fb9-4426-be39-15f4ede9b10d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If we want to have multiple filters applied as AND, use & between them\n",
    "# If we want to have multiple filters applied as OR, use | between them\n",
    "is_truck = cars.car_type == 'truck'\n",
    "\n",
    "# If we want to sort values, we can use the .sort_values() method\n",
    "# to list one or more columns to sort by, sequentially.\n",
    "# The following sorting should provide black vehicles made in 2023 first, and white vehicles in 2017 last.\n",
    "cars[is_truck].sort_values(by=[\"color\", \"year\"], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec6023-e6af-44c3-a718-6b4f02037c41",
   "metadata": {},
   "source": [
    "#### **Exercise**\n",
    "Using what we learned about filters and masks, can you filter this dataset to only include cars that are under $20,000 or otherwise can be financed (not leased)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db665a3c-d144-466d-83ff-08411fa09e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Exercise Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf59c4-173a-4378-9c87-232c70ea087c",
   "metadata": {},
   "source": [
    "#### Example 3.2: Transforming DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a8a78-443e-4cbe-bd78-6cc85d1433ba",
   "metadata": {},
   "source": [
    "We can transform datasets in a number of ways in Pandas, but the main rule you want to remember is whether a Pandas method will transform a dataset by mutating it, or return a copy of a dataframe with a transformation. When in doubt, Pandas DOES have a `.copy()` method to DataFrames so that you can always have a clean copy to work with.\n",
    "\n",
    "So what are some of the ways we can transform data in Pandas?\n",
    "1. We can apply a vectorized operation on the dataframe itself for certain mathematical or simple logical operations by just writing the operation literally.\n",
    "2. We can apply a change iteratively using `.apply()`, which is much slower because it's not vectorized. Alternatively, you can consider leveraging NumPy's `np.vectorize` on a callback function and then pass in the appropriate columns/rows that you would have accessed in the `.apply()` (See [this Medium article](https://michael-taverner.medium.com/stop-using-apply-and-start-using-numpys-vectorize-d589d15bc77b))\n",
    "3. We can merge datasets horizontally (add new columns) using the `.merge()` method, which is Pandas' equivalent of a JOIN operation in SQL.\n",
    "4. We can append datasets vertically (combine several DataFrames that have the same schema) using `.concat()` method, which is Pandas' equivalent of a UNION or UNION ALL operation in SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4509e56f-6630-4c26-bad9-fe98dde1a523",
   "metadata": {},
   "source": [
    "**Testing out various element-wise operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7508ae-6226-4b5b-802a-e9d41f765f0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vectorized operation v. apply v. np.vectorize\n",
    "@timeit\n",
    "def apply_discount_math(df, pct_off=20, old_col=\"price_range\", new_col=\"discounted_price_range\"):\n",
    "    new = df.copy()\n",
    "    new[new_col] = new[old_col] * (100 - pct_off)/(100)\n",
    "    return new\n",
    "\n",
    "@timeit\n",
    "def apply_discount_pandas_apply(df, pct_off=20, old_col=\"price_range\", new_col=\"discounted_price_range\"):\n",
    "    new = df.copy()\n",
    "    # What the hell is a lambda? It's an anonymous function! The JS equivalent\n",
    "    # would be just creating in-line an arrow function or an unnamed function like\n",
    "    # (x) => x[old_col] * (100 - pct_off)/(100) (remember, old_col and pct_off are parent-scoped)\n",
    "    # x, combined with the axis = 1 represents a single row of the dataframe (kind of an object/dictionary)\n",
    "    new[new_col] = new.apply(lambda x: x[old_col] * (100 - pct_off)/(100), axis=1)\n",
    "    return new\n",
    "\n",
    "@timeit\n",
    "def apply_discount_numpy_vectorize(df, pct_off=20, old_col=\"price_range\", new_col=\"discounted_price_range\"):\n",
    "    def apply_discount(old_price, pct_off):\n",
    "        return old_price * (100 - pct_off)/(100)\n",
    "    new = df.copy()\n",
    "    apply_discount = np.vectorize(apply_discount)\n",
    "    new[new_col] = apply_discount(new[old_col], pct_off)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d5990a-406e-4505-9ad7-60d13bad2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "math = apply_discount_math(cars)\n",
    "math.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac6d41-321d-409e-995f-980afddf8080",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_apply = apply_discount_pandas_apply(cars)\n",
    "pd_apply.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087fd58-b8ab-43b3-bba6-c571a39cf038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_vectorize = apply_discount_numpy_vectorize(cars)\n",
    "np_vectorize.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1f4c9-5e90-4f3e-8321-051a8feddfde",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e101c-47f1-4f18-af94-cb61276f12ea",
   "metadata": {},
   "source": [
    "Now that you've seen the three different ways to create a new DataFrame column, can you create a new column, \"price_range_after_financing\", that will capture the following logic?\n",
    "1. If the finance_option is \"lease\", add a 5% premium (so 105% of the cost) to the price_range\n",
    "2. If the finance_option is \"finance\", add a 10% premium (so 110% of the cost) to the price_range\n",
    "3. If the finance_option is \"cash\", keep the price_range the same since you're paying for it outright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6a62d-ff85-4ecb-b181-887e6010b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7d409-9019-47c7-a586-eeb8892247c5",
   "metadata": {},
   "source": [
    "**Merging datasets**\n",
    "\n",
    "Say that we have another dataset that represents paint vendors when you have to repair paint damage on the car, and that orange and green are premium colors that have to go to specific vendors, otherwise, the manufacturer will deal with them.\n",
    "\n",
    "We can use a `.merge()` to join these two datasets together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2124dc-6880-4973-954e-b38074724bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paint_vendors = pd.DataFrame({ \"color\": [\"orange\", \"green\"], \"paint_vendor\": [\"Cuztom Colorz\", \"PNW Verdantry\"]})\n",
    "paint_vendors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc9698-99d4-435b-913a-d5e852bf4cde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cars_with_vendors = cars.merge(paint_vendors, how=\"left\", on=\"color\")\n",
    "cars_with_vendors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5784ae2-25f8-424a-8f69-a4cc0ebb81a5",
   "metadata": {},
   "source": [
    "But in the above, you see we have a bunch of NaN results! How do we backfill them so that they show the manufacturer instead? There's another function, `.fillna()` that is helpful here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e90f1-3aed-47a2-814b-fa45aff16f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_with_vendors_clean = cars_with_vendors.copy()\n",
    "cars_with_vendors_clean[\"paint_vendor\"] = cars_with_vendors_clean[\"paint_vendor\"].fillna(cars_with_vendors[\"make\"])\n",
    "cars_with_vendors_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855363e-b7b8-4624-bcf3-cf4e70d94549",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235218a9-97bb-47d6-a995-4ffe6f534967",
   "metadata": {},
   "source": [
    "Now that you have a sense of how `.merge()` is used in Pandas, we want to merge this `cars` dataset with another dataset, `potential_buyers`, where a buyer has identified a specific interest in a single car_type for each row of data. Can we merge these two datasets and ONLY show the rows of data that actually have a match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee50d1-ad17-4479-8dd3-307cfd199165",
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_buyers = pd.DataFrame({\n",
    "    \"car_type\": [\"truck\", \"suv\", \"suv\", \"hatchback\", \"sedan\", \"sedan\", \"hatchback\", \"truck\"],\n",
    "    \"buyer\": [\"Sam\", \"Sam\", \"Chris\", \"Alex\", \"Alex\", \"Dee\", \"Chris\", \"Dee\"]\n",
    "})\n",
    "potential_buyers\n",
    "# Exercise Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab076c4c-dd20-478e-b473-98ed6279a14c",
   "metadata": {},
   "source": [
    "**Combining datasets through a concat**\n",
    "\n",
    "Combining several Pandas DataFrames using a concat is dead-simple, assuming that the columns for each dataset match correctly, in sequence and with indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7e848-6d02-4609-9696-67180d547688",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_5 = create_randomized_cars_dataset(5)\n",
    "cars_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc02338-bac5-4b57-8190-fd1f3c9d8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_10 = create_randomized_cars_dataset(10)\n",
    "cars_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7704812-1947-4c93-91de-8c5fd6f8c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "union = pd.concat([cars_5, cars_10], ignore_index=True)\n",
    "union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae3019-899a-40e3-a723-c647a26e8c86",
   "metadata": {},
   "source": [
    "#### Example 3.3: Group Bys and Aggregating Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed6ef14-7f33-47bc-a149-179ae37bb3a9",
   "metadata": {},
   "source": [
    "There are a ton of aggregation operations that are available via Pandas, and we won't go into much detail here, but things like count, sum, etc. are things that you can do across your entire dataset, or with grouping by particular column values or groups of column values using the `.groupby()` method before the aggregation you're attempting to run. If you're looking for a suite of statistical aggregations all at once (and your dataset is appropriate for them, you can also check out the `.describe()` method, which will give you min, max, standard deviations, counts, and percentile values. [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html)\n",
    "\n",
    "And, if you want to get even MORE generic with aggregations, check out the `.agg()` method, which lets you apply more general Python methods as your aggregation. [https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04074a-1f3e-4754-9873-c4aa0de8ed22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting a count based on color\n",
    "cars.groupby(\"color\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f870b1-a35a-4935-9a48-bfcbae0b552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the make based on color\n",
    "cars.groupby(\"color\")[\"make\"].apply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa4a91-d677-487b-bba2-3b7197d6fbc0",
   "metadata": {},
   "source": [
    "### 4. Let's play with survey data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65caa6f0-c7dc-4e76-9985-9c4c6f79b032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "574abe56-e730-4036-b41c-817c56be55ef",
   "metadata": {},
   "source": [
    "### 5. Visualizations with seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2bfaa1-3265-47d8-9434-7c6ecb3fc7b0",
   "metadata": {},
   "source": [
    "TBD in Part 2 of this series!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
